**Useful links**

https://github.com/PritiG1/Multimodal-RAG/blob/main/src/chunk_embed.py

https://medium.com/@pritigupta.ds/docling-powered-rag-querying-over-complex-pdfs-d99f5f58bc33


CHATGPT CONV
https://chatgpt.com/share/68b00262-4898-800b-8f7e-23b10967c6fa


**RAG OVER COMPLEX MULTIMODAL MARKDOWN PDFS w/ docling**

----------------------
â¸»

1. Why RAG in this context?

Think of your quarterly reports:
	â€¢	Each report can be dozens of pages, sometimes with text + tables + images.
	â€¢	You want to pull specific fields (NAV, valuation date, commitments, distributions, fees, etc.).
	â€¢	If you naively send the whole document to the LLM for extraction:
	â€¢	You hit the context window limit (10kâ€“200k tokens depending on the model).
	â€¢	You pay more in tokens and latency.
	â€¢	The modelâ€™s accuracy drops, because it has to search a haystack of irrelevant text.

ğŸ‘‰ RAG solves this:
Instead of sending everything, you retrieve only the chunks that are most likely to contain each field. That way:
	â€¢	You stay under context budget.
	â€¢	The LLM sees only relevant evidence, boosting precision.
	â€¢	It scales to hundreds of reports over time.


**STEPS FOR THIS (BASELINE)**


1) Ingest & normalize (Markdown + external images)
	â€¢	Parse Markdown structurally (donâ€™t treat it as plain text): keep block boundaries, headings (#..####), lists, code fences, and tables. Persist a clean AST plus raw text for each node.
	â€¢	Link + image resolution: for each ![alt](url) store {doc_id, section_path, img_url, alt}.
	â€¢	If images can contain values you need (tables/figures), run captioning + OCR at ingest and store the text alongside the chunk (cheap at query-time). BLIP-2 is a strong captioner; pair with OCR (Tesseract or docTR) for text in charts/tables.  ï¿¼ ï¿¼ ï¿¼
	â€¢	Housekeeping: strip boilerplate (TOCs, disclaimers), normalize whitespace/currency symbols, fix malformed tables, hash each node for dedupe, and record byte/line offsets for provenance.

2) Chunking (Markdown-aware, table-safe)
	â€¢	Split on headings first, then enforce a max token window (e.g., 400â€“800 tokens) with ~10â€“15% overlap. Keep tables whole with their header row. Libraries like LangChainâ€™s MarkdownHeaderTextSplitter or LlamaIndexâ€™s Markdown node parsers are good references.  ï¿¼ ï¿¼ ï¿¼
	â€¢	Metadata: store doc_id, section_path (e.g., Overview/NAV/2025-Q2), token_count, and any img_caption_ocr strings tied to this chunk.

3) Index (hybrid retrieval + optional reranking)
	â€¢	Hybrid search reliably beats â€œdense onlyâ€ or â€œBM25 onlyâ€ for messy, financial text: build BM25 for exact term recall + a dense index (FAISS/Qdrant) for semantics; combine scores or do reciprocal-rank fusion.  ï¿¼ ï¿¼
	â€¢	Rerank (optional but effective): apply a cross-encoder (e.g., BAAI bge-reranker) on the top-K to sharpen precision before you spend tokens. Be mindful of cost at scale.  ï¿¼ ï¿¼
	â€¢	Images: index captions/OCR text alongside the same chunk_id. If you truly need visual search, add a CLIP image-embedding index keyed to the chunk.  ï¿¼

4) Retrieval (field-scoped, budget-aware)

For each target field in your schema (e.g., NAV_Date, NAV_Value, Commitments_QTD, Distributions_QTD, Management_Fee):
	â€¢	Build field-specific queries + synonyms:
	â€¢	NAV_Date: â€œNAV as ofâ€, â€œvaluation dateâ€, â€œNAV dateâ€, â€œNAV @â€.
	â€¢	Commitments: â€œcapital commitmentsâ€, â€œtotal commitmentsâ€, â€œcommitted capitalâ€, etc.
	â€¢	Stage 1 (recall): fetch ~20â€“50 candidates via hybrid retrieval; filter by section_path patterns (e.g., â€œFinancialsâ€, â€œCapital Accountâ€, â€œManagement Feeâ€).
	â€¢	Stage 2 (precision): rerank to top 3â€“6 chunks, ensuring total â‰¤ your token budget (e.g., <2â€“3k tokens/extraction call). Maintain a fieldâ†’chunk map so different fields can reuse overlapping chunks without duplicate tokens.  ï¿¼ ï¿¼
	â€¢	Images: by default pass caption/OCR text. Only pass the image itself to a multimodal model if the value clearly depends on a figure/screenshot table.  ï¿¼

5) Extraction (strict schema, citations, retries)
	â€¢	Use Structured Outputs (JSON Schema) so the model must return exactly your schema. Provide enums, formats (date, currency), and examples. Validate with jsonschema/Pydantic; retry with a shorter prompt if validation fails.  ï¿¼ ï¿¼ ï¿¼
	â€¢	Mapâ†’Reduce extraction:
	â€¢	Map: run extraction per retrieved chunk (cheap, parallel).
	â€¢	Reduce: merge candidates per field using confidence + business rules; keep provenance {doc_id, section_path, line_offset | table_ref | img_url}. This mirrors standard map-reduce summarization patterns.  ï¿¼
	â€¢	Confidence: combine (a) reranker score, (b) lexical hits (e.g., presence of â€œas ofâ€), (c) JSON Schema validation success, and (d) optional LLM self-estimated confidence.

6) Keep context under control
	â€¢	Never stuff full reports. Impose hard caps: e.g., max 6 chunks per call, â‰¤3k input tokens.
	â€¢	Add few, surgical exemplars (1â€“2) and compress long instruction blocks with LLMLingua / LongLLMLingua / LLMLingua-2 to preserve guidance while fitting the window.  ï¿¼
	â€¢	Cache per-section headnotes (short factual bullet digests) at ingest; retrieve headnotes instead of full text unless ambiguity remains.

7) Post-processing & QA (finance-specific)
	â€¢	Normalization: unify currencies (store value + currency), parse NAV_Date to ISO-8601, parse human numerals (â€œâ‚¬1.2bnâ€â†’1200000000 w/ magnitude=bn).
	â€¢	Arithmetic checks:
	â€¢	Ending_NAV = Beginning_NAV + Contributions â€“ Distributions Â± P&L Â± Fees (tolerances for rounding).
	â€¢	Quarter deltas consistent with YTD.
	â€¢	Provenance first-class: attach the exact source section lines (and image URL if used) for each field â†’ makes audits painless.

8) When to send pixels (vs. text about pixels)
	â€¢	Default: only pass caption/OCR strings from imagesâ€”nearly free in tokens and good enough for most tables/charts.
	â€¢	Send the image only when (detectors): (i) chunk has a table screenshot with numbers but weak OCR confidence, (ii) caption is vague, or (iii) the value is visually encoded (e.g., a figure footnote). Use a multimodal call for just those fields.  ï¿¼

â¸»

Practical wiring (minimal, production-lean)

Data model

Chunk(id, doc_id, section_path, text, tokens, img_caption_ocr:list[str], offsets, hash)
ImageRef(doc_id, section_path, url, alt, caption, ocr_text)

Index
	â€¢	BM25: Elastic/Lucene or Tantivy.
	â€¢	Dense: FAISS (IVF/HNSW) or Qdrant; store embedding + chunk_id.  ï¿¼
	â€¢	(Optional) Rerank: BAAI/bge-reranker-large as a service; score top-K then keep top-M.  ï¿¼

Retrieval per field

query = field_templates[field_name](synonyms, company, quarter, doc_date)
cands = hybrid_search(query, top_k=50) â†’ filter by section_path â†’ rerank â†’ top_m=3â€“6

Extraction call (strict JSON)
	â€¢	Use OpenAI Structured Outputs with your JSON Schema (or Pydantic model_json_schema()), set response format to schema; include just the selected chunks, a short instruction, 1â€“2 examples, and a hard max_input_tokens. Validate â†’ retry on fail.  ï¿¼

Aggregation
	â€¢	If multiple chunks claim the same field: prefer (1) exact phrase anchors (â€œNAV as ofâ€), then (2) highest rerank score, then (3) most recent date within quarter; attach all candidate provenances for audit.

QA rules (examples)
	â€¢	Dates inside quarter; currency consistent with company base; MgmtFee ~ mgmt_rate Ã— committed/invested within tolerance; reject if any required term missing in the source snippet.

Logging
	â€¢	Save the prompt hash, selected chunk ids, and schema validation events. This makes drift, failures, and cost visible.

â¸»

What you gain vs. â€œsend the whole reportâ€
	â€¢	Scales to hundreds of reports without blowing context windows (hard caps + hybrid retrieval).  ï¿¼
	â€¢	Higher precision on the right sections (cross-encoder reranking when needed).  ï¿¼
	â€¢	Deterministic outputs via Structured Outputs + JSON Schema validation.  ï¿¼
	â€¢	Token efficiency via LLMLingua-style prompt compression for instructions/examples.  ï¿¼
	â€¢	Images handled without always paying multimodal costs (caption/OCR first, pixels only on demand).  ï¿¼

â¸»

If you want, I can tailor this to your exact schema (e.g., your NAV_Date, Valuation_Date, FONDO_TARGET_ASSET, etc.) and sketch the concrete Python code (ingestion, hybrid retrieval, structured extraction, validators).